# Tokenization in LLMs — A Code-First Guide

This repository demonstrates the **5 tokenization methods** used in Large Language Models (LLMs).  
It focuses on helping learners see **how text is converted into tokens and token IDs** across different approaches.

## What’s Inside

The notebook explores five tokenization strategies:

1. **Character-level** — every character is a token.
2. **Word-level** — splits text into words (prone to OOV issues).
3. **WordPiece (BERT-style)** — subword tokens for better vocabulary coverage.
4. **SentencePiece / Unigram** — language-agnostic subwords suitable for multilingual text.
5. **Byte-level BPE (GPT-style)** — robust byte-level tokens, no OOV.


