# From-Characters-to-Bytes-Basic-Tokenization-Strategies-in-Large-Language-Models
This repository explores how Large Language Models tokenize text, starting from character-level tokenization and progressing through word-level, subword (WordPiece), SentencePiece, and byte-level BPE tokenization.  Each method is demonstrated with runnable Python code using the same input text
